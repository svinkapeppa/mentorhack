{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.models.wrappers.fasttext import FastText as FT_wrapper\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "def mystem_analyze(str):\n",
    "    global m\n",
    "    try:\n",
    "        return m.analyze(str)\n",
    "    except BrokenPipeError as ex:\n",
    "        m = Mystem()\n",
    "        return mystem_analyze(str)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "DATASETS = Path('~/data/taskdialog').expanduser()\n",
    "MODELS = Path('~/data/taskdialog/models').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_forum(f):\n",
    "    try:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i += 1\n",
    "            t = '\\n'.join(' '.join(wordpunct_tokenize(l)) for l in json.loads(line)['text'].strip().splitlines() if l.strip())\n",
    "            if t and len(t) < 4000:\n",
    "                yield t\n",
    "            if i % 1000000 == 0:\n",
    "                print(i)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "CPU times: user 31.4 s, sys: 908 ms, total: 32.3 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with (DATASETS / 'forummoskva.jsonl').open() as f:\n",
    "    texts = preprocess_forum(f)    \n",
    "    negatives.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "droms = []\n",
    "with (DATASETS / 'dromru.jsonl').open() as f:\n",
    "    texts = preprocess_forum(f)    \n",
    "    droms.extend(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(droms)\n",
    "del droms[10*1000*1000:]\n",
    "negatives.extend(droms)\n",
    "del droms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.28 s, sys: 360 ms, total: 5.64 s\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with (DATASETS / 'vashdomru.jsonl').open() as f:\n",
    "    texts = preprocess_forum(f)    \n",
    "    negatives.extend(texts)  \n",
    "#     print(len(texts))  # 300k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.84 s, sys: 344 ms, total: 7.18 s\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with (DATASETS / 'antiwomenru.jsonl').open() as f:\n",
    "    texts = preprocess_forum(f)    \n",
    "    negatives.extend(texts)  \n",
    "#     print(len(texts))  # 1.8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "CPU times: user 7min 10s, sys: 16.3 s, total: 7min 27s\n",
      "Wall time: 7min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with (DATASETS / 'womenru.jsonl').open() as f:\n",
    "    texts = preprocess_forum(f)    \n",
    "    negatives.extend(texts)  \n",
    "#     print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24440886\n"
     ]
    }
   ],
   "source": [
    "print(len(negatives))\n",
    "random.shuffle(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_imperative(word):\n",
    "    try:\n",
    "        p = [pm for pm in morph.parse(word) if 'INFN' in pm.tag][0]\n",
    "    except IndexError as ex:\n",
    "        return\n",
    "    try:\n",
    "        sing = p.inflect({'VERB', 'perf', 'impr', 'excl', 'plur'}).word\n",
    "        plur = p.inflect({'VERB', 'perf', 'impr', 'excl'}).word\n",
    "        return (sing, plur)\n",
    "    except AttributeError as ex:\n",
    "        return\n",
    "    \n",
    "assert to_imperative('Совершить') == ('совершите', 'соверши')\n",
    "    \n",
    "words_of_need = {'необходимо', 'нужно', 'требуется'}\n",
    "\n",
    "def get_imperative_variants(text):\n",
    "    try:\n",
    "        words = []\n",
    "        isinf = []\n",
    "        for tok in mystem_analyze(text.lower()):\n",
    "            if 'analysis' in tok:\n",
    "                if len(tok['analysis']) >= 1:\n",
    "                    gram = tok['analysis'][0]['gr']\n",
    "                    infinitive = 'инф' in gram and 'V' in gram\n",
    "                    w = tok['text']\n",
    "                    words.append(w)\n",
    "                    isinf.append(infinitive)\n",
    "        if max(isinf) == False:\n",
    "            return text,\n",
    "        if isinf[0]:\n",
    "            variants = []\n",
    "            for w, v in zip(words, isinf):\n",
    "                if v:\n",
    "                    imp = to_imperative(w)\n",
    "                    variants.append(imp or (w, w))\n",
    "                else:\n",
    "                    variants.append((w, w))\n",
    "            sing, plur = zip(*variants)\n",
    "            return text, ' '.join(sing), ' '.join(plur)\n",
    "        elif words_of_need.intersection(words):\n",
    "            prev_word = ''\n",
    "            variants = []\n",
    "            used_imperative = False\n",
    "            for w, v in zip(words, isinf):\n",
    "                if v and prev_word in words_of_need:\n",
    "                    imp = to_imperative(w)\n",
    "                    if imp:\n",
    "                        variants.pop()\n",
    "                        variants.append(imp)\n",
    "                        used_imperative = True\n",
    "                    else:\n",
    "                        variants.append((w, w))\n",
    "\n",
    "                else:\n",
    "                    variants.append((w, w))\n",
    "                prev_word = w\n",
    "\n",
    "            if used_imperative:\n",
    "                sing, plur = zip(*variants)\n",
    "                return text, ' '.join(sing), ' '.join(plur)\n",
    "    except ValueError as ex:\n",
    "        pass\n",
    "    return text,\n",
    "        \n",
    "assert get_imperative_variants('нужно сделать хорошо')[1] == 'сделайте хорошо'\n",
    "assert get_imperative_variants('повертеть попой')[2] == 'поверти попой'\n",
    "assert len(get_imperative_variants('хорошо сделать')) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "positives = []\n",
    "with (DATASETS / 'youdo.txt').open() as f:\n",
    "    csvr = csv.reader(f, delimiter=',')\n",
    "    for title, body in csvr:\n",
    "        t = '\\n'.join(' '.join(wordpunct_tokenize(l)) for l in body.strip().splitlines() if l)\n",
    "        positives.append((title, t))\n",
    "        \n",
    "with (DATASETS / 'fl.csv').open() as f:\n",
    "    c = csv.reader(f)\n",
    "    positives.extend(c)  \n",
    "    \n",
    "random.shuffle(positives)\n",
    "\n",
    "train_tasks_n = int(len(positives) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_train = [(title, body) for title, body in positives[:train_tasks_n] if title.strip() and body.strip()]\n",
    "positives_test = [(title, body) for title, body in positives[train_tasks_n:] if title.strip() and body.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1bda7cabb4a9>\u001b[0m in \u001b[0;36mget_imperative_variants\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misinf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprev_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_of_need\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mimp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_imperative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0mvariants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1bda7cabb4a9>\u001b[0m in \u001b[0;36mto_imperative\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'perf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'impr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'excl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plur'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mplur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'perf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'impr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'excl'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.5/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36minflect\u001b[0;34m(self, required_grammemes)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_grammemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_grammemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.5/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m_inflect\u001b[0;34m(self, form, required_grammemes)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_grammemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         possible_results = [f for f in self.get_lexeme(form)\n\u001b[0m\u001b[1;32m    295\u001b[0m                             if required_grammemes <= f[1].grammemes]\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.5/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mget_lexeme\u001b[0;34m(self, form)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mmethods_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlast_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethods_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_method\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lexeme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.5/site-packages/pymorphy2/units/by_lookup.py\u001b[0m in \u001b[0;36mget_lexeme\u001b[0;34m(self, form)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mnew_methods_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethods_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpara_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_form\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_methods_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "augmented_body = [get_imperative_variants(body) for title, body in positives_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 53 s, total: 1min 14s\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(processes=40) as pool:\n",
    "    augmented_body = pool.map(get_imperative_variants, (body for title, body in positives_train[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 13 s, total: 1min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with (DATASETS / 'tasks_train.csv').open('w') as f:\n",
    "    c = csv.writer(f)\n",
    "    for (title, body), augs in zip(positives_train, augmented_body):\n",
    "        for a in augs:\n",
    "            c.writerow([title, a])\n",
    "\n",
    "with (DATASETS / 'tasks_test.csv').open('w') as f:\n",
    "    c = csv.writer(f)\n",
    "    for title, body in positives_test:\n",
    "        c.writerow([title, body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пропала кошка в Красногорске . Район Медучилище . Срочно нужно пару человек на поиски . Подробности по телефону . За нахождение отдельное вознаграждение . Поиск будет вестись в окрестностях указанного адреса .\\nБолее подробно условия задания обсудим с исполнителем .\\nВ предложениях указывайте сроки , когда сможете выполнить задание и цену за работу .\\nЖду Ваших предложений !'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_body[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 29.1 s, total: 3min 51s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "negatives_train_n = int(len(negatives) * 0.8)\n",
    "train = [(0, body) for body in negatives[:negatives_train_n]] + [(1, body[0]) for body in augmented_body]\n",
    "random.shuffle(train)\n",
    "\n",
    "with (DATASETS / 'train4.tsv').open('w') as f:\n",
    "    csvw = csv.writer(f, delimiter='\\t')\n",
    "    for label, body in train:\n",
    "        csvw.writerow([label, body])\n",
    "\n",
    "        \n",
    "test = [(0, body) for body in negatives[negatives_train_n:]] + [(1, body) for title, body in positives_test]\n",
    "random.shuffle(test)\n",
    "with (DATASETS / 'test4.tsv').open('w') as f:\n",
    "    csvw = csv.writer(f, delimiter='\\t')\n",
    "    for label, body in test:\n",
    "        csvw.writerow([label, body])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 348 ms, sys: 75.9 ms, total: 424 ms\n",
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def features(s):\n",
    "    for g in mystem_analyze(s):\n",
    "        if 'analysis' in g:\n",
    "            if len(g['analysis']) > 0:\n",
    "                for gr in g['analysis']:\n",
    "                    if 'пов' in gr['gr']:\n",
    "                        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "for i in range(10000):\n",
    "    features('Вася , реализовать функционал . Петя напишет план а ты функционал')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []\n",
    "train_y = []\n",
    "train_features = []\n",
    "with (DATASETS / 'train4.tsv').open() as f:\n",
    "    c = csv.reader(f, delimiter='\\t')\n",
    "    for label, body in c:\n",
    "        body = body[:200]\n",
    "        if ' ' not in body:\n",
    "            continue\n",
    "        body = body[:body.rindex(' ')]\n",
    "#         train_features.append(features(body))\n",
    "        train_text.append(body)\n",
    "        train_y.append(int(label))\n",
    "\n",
    "test_text = []\n",
    "test_y = []\n",
    "test_features = []\n",
    "with (DATASETS / 'test4.tsv').open() as f:\n",
    "    c = csv.reader(f, delimiter='\\t')\n",
    "    for label, body in c:\n",
    "        body = body[:200]\n",
    "        if ' ' not in body:\n",
    "            continue\n",
    "        body = body[:body.rindex(' ')]\n",
    "#         test_features.append(features(body))\n",
    "        test_text.append(body)\n",
    "        test_y.append(int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with (DATASETS / 'dataset.txt').open('w') as f:\n",
    "#     for label, text in data:\n",
    "#         print(text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_n = int(len(data) * 0.8)\n",
    "# count = 0\n",
    "# with (DATASETS / 'train2.tsv').open('w') as f:\n",
    "#     csvw = csv.writer(f, delimiter='\\t')\n",
    "#     for label, text in data[:train_n]:\n",
    "#         count += 1\n",
    "#         if count % 20000 == 0:\n",
    "#             print('{:.2f}%'.format(count/train_n*100))\n",
    "#         if label == '1':\n",
    "#             for augmented_text in get_imperative_variants(text):\n",
    "#                 csvw.writerow([label, augmented_text])\n",
    "#         else:\n",
    "#             csvw.writerow([label, text])\n",
    "\n",
    "# with (DATASETS / 'test.tsv').open('w') as f:\n",
    "#     csvw = csv.writer(f, delimiter='\\t')\n",
    "#     for label, text in data[train_n:]:\n",
    "#         csvw.writerow([label, text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = '''форумчане\n",
    "форумчане\n",
    "simplexman\n",
    "saitov\n",
    "дрома\n",
    "drom\n",
    "quote\n",
    "дром\n",
    "професcиональный\n",
    "рекомендую\n",
    "советую\n",
    "взаимосвязями\n",
    "тебе\n",
    "загляни\n",
    "youdo\n",
    "фрилансеры'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 3s, sys: 1min 4s, total: 20min 8s\n",
      "Wall time: 20min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_path = MODELS / 'tfidf5.pickle'\n",
    "if tfidf_path.exists():\n",
    "    with tfidf_path.open('rb') as f:\n",
    "        tfidf = pickle.load(f)\n",
    "else:\n",
    "    tfidf = TfidfVectorizer(min_df=5, stop_words=stopwords)\n",
    "    tfidf.fit(train_text)\n",
    "    with tfidf_path.open('wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "\n",
    "train_X = tfidf.transform(train_text)\n",
    "test_X = tfidf.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 11.2 s, total: 2min 13s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])\n",
    "with (MODELS / 'classifier5.pickle').open('wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  \t0.9923632064836054\t0.9942928958053494\n",
      "Pos Accuracy\t1.0\t1.0\n",
      "Neg Accuracy\t1.0\t1.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy  ', (classifier.predict(test_X) == test_y).mean(), (classifier.predict(train_X) == train_y).mean(), sep='\\t')\n",
    "test_pos = test_y == 1\n",
    "train_pos = train_y == 1\n",
    "print('Pos Accuracy', (classifier.predict(train_X[train_pos]) == train_y[train_pos]).mean(), (classifier.predict(test_X[test_pos]) == test_y[test_pos]).mean(), sep='\\t')\n",
    "print('Neg Accuracy', (classifier.predict(train_X[~train_pos]) == train_y[~train_pos]).mean(), (classifier.predict(test_X[~test_pos]) == test_y[~test_pos]).mean(), sep='\\t')\n",
    "\n",
    "# 1\n",
    "#   Accuracy\t0.9966766041670294\t0.9979463617801813\n",
    "# Pos Accuracy\t0.974006711709939\t0.9588633251535406\n",
    "# Neg Accuracy\t0.9993740346497515\t0.9989338185724046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2words = {i: w for w, i in tfidf.vocabulary_.items()}\n",
    "words = []\n",
    "for i in range(classifier.coef_.shape[1]):\n",
    "    words.append((id2words[i], classifier.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = sorted(words, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "importances[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = FT_wrapper.train('/home/marat/fastText-0.1.0/fasttext', corpus_file=str(DATASETS / 'dataset.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.save('/home/marat/data/taskdialog/models/fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5d9a669af868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'youdo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "model_wrapper.most_similar('youdo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
